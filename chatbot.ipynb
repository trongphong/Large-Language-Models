{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 15859,
     "status": "ok",
     "timestamp": 1732019360413,
     "user": {
      "displayName": "Phong Nguyen Trong",
      "userId": "11475700421778825460"
     },
     "user_tz": -60
    },
    "id": "CVXMvZTcUCmI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1732019307917,
     "user": {
      "displayName": "Phong Nguyen Trong",
      "userId": "11475700421778825460"
     },
     "user_tz": -60
    },
    "id": "0Gvl3KG1UCmK",
    "outputId": "ca175b8e-bcfc-4270-b5e1-ebb1ec7042d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dNtjtq9JUCmK"
   },
   "outputs": [],
   "source": [
    "check_point=\"meta-llama/Llama-3.2-1B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xqtgdK8jUCmL"
   },
   "outputs": [],
   "source": [
    "model=AutoModelForCausalLM.from_pretrained(check_point,torch_dtype=torch.bfloat16, token=token_acc).to(device)\n",
    "tokenizer=AutoTokenizer.from_pretrained(check_point,token=token_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VYd3xwyaUCmM",
    "outputId": "9ad450fd-c422-45d6-b869-5242e66b86d2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 14 Dec 2024\\n\\nChatBot<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHow are you<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nI'm doing well, thanks for asking. I'm a large language model, so I don't have feelings or emotions like humans do, but I'm functioning properly and ready to help with any questions or topics you'd like to discuss. How about you? How's your day going?<|eot_id|>\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test\n",
    "text='How are you'\n",
    "prompt='ChatBot'\n",
    "test = [\n",
    "    {\"role\": \"system\", \"content\": prompt},\n",
    "    {\"role\": \"user\", \"content\": text},\n",
    "]\n",
    "test_token=tokenizer.apply_chat_template(\n",
    "        test,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors='pt').to(device)\n",
    "test_gen=model.generate(\n",
    "        test_token,\n",
    "        max_length=100,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        top_k=50,\n",
    "        temperature=0.0001\n",
    "        )\n",
    "tokenizer.decode(test_gen[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "y8iR1gKjUCmM",
    "outputId": "645fe12a-9a84-4bcf-fcd0-f519c8162ce9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 14 Dec 2024\n",
      "\n",
      "ChatBot<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "How are you<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "I'm doing well, thanks for asking. I'm a large language model, so I don't have feelings or emotions like humans do, but I'm functioning properly and ready to help with any questions or topics you'd like to discuss. How about you? How's your day going?<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(test_gen[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "EsNOB568UCmM"
   },
   "outputs": [],
   "source": [
    "def func_chat(text, sys_prompt='Welcome'):\n",
    "    messages =[\n",
    "        {\"role\": \"system\", \"content\": sys_prompt},\n",
    "\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "    messages_token=tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        #continue_final_message=False,\n",
    "        return_tensors='pt'\n",
    "        ).to(device)\n",
    "    messages_gen=model.generate(\n",
    "        messages_token,\n",
    "        max_length=1000,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        top_k=50,\n",
    "        temperature=0.0001,\n",
    "        #pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    ouput=tokenizer.decode(messages_gen[0], skip_special_tokens=True)\n",
    "    return ouput.split('assistant')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "P2EpjaceUCmM",
    "outputId": "3ad972e9-bc73-44bd-c40f-bce5624136be"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nParis is the capital city of France, a country located in Western Europe. It is situated in the Île-de-France region, which is one of the largest metropolitan areas in Europe.\\n\\nParis is located in the north-central part of France, approximately 300 kilometers (186 miles) east of the English Channel. The city is nestled between the Seine River and the Marne River, and it is surrounded by several other major cities, including:\\n\\n* Lyon (to the south)\\n* Marseille (to the south)\\n* Nice (to the southeast)\\n* Strasbourg (to the east)\\n* Reims (to the northeast)\\n\\nParis is a major hub for international business, culture, and tourism, and it is known for its iconic landmarks, such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral.\\n\\nHere's a rough outline of Paris's location:\\n\\n* Latitude: 48.8567° N\\n* Longitude: 2.3522° E\\n* Elevation: 48 meters (157 feet) above sea level\\n* Nearest airport: Charles de Gaulle Airport (CDG)\\n* Nearest city: Lyon (approximately 300 kilometers / 186 miles away)\\n\\nI hope this helps you visualize the location of Paris!\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_chat('where is Paris', 'chat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "qhnb2hmpUCmM"
   },
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "8wsJz3UuUCmN",
    "outputId": "4d2b795f-6020-493d-b1cd-5157a1998b55"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phong/miniconda3/envs/mlenv/lib/python3.12/site-packages/gradio/components/chatbot.py:225: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "demo=gr.ChatInterface(\n",
    "    fn=func_chat,\n",
    "    #type='messages',\n",
    "    textbox=gr.Textbox(placeholder='Enter question here', container=False, scale=7),\n",
    "    chatbot=gr.Chatbot(height=400),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNcgh35OUCmN",
    "outputId": "e548a79d-5870-4838-d867-ae2498dee8e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "crJtDt9cUCmN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
